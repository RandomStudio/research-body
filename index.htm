<h2>
<a id="user-content-title" class="anchor" href="#title" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TITLE</h2>
<p>"Body": using your self as a search tool</p>
<h2>
<a id="user-content-blurb" class="anchor" href="#blurb" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BLURB</h2>
<p>We don't count any data scientists in our team (yet) but this
project earned us some valuable experience applying Machine Learning tools in creative applications.</p>
<h2>
<a id="user-content-body-text" class="anchor" href="#body-text" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>BODY TEXT</h2>
<h3>
<a id="user-content-whats-the-deal-with-ai" class="anchor" href="#whats-the-deal-with-ai" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What's the deal with AI?</h3>
<p>Artifical Intelligence (AI) technologies get thoroughly hyped (perhaps justifiably) in the media. What is less readily understood - at least by the general public - is that a lot of the recent progress has been less about general-purpose intelligence (machines that can truly "think", whatever that means) and more about a much more narrowly-defined field known as <a href="https://en.wikipedia.org/wiki/Machine_learning" rel="nofollow">Machine Learning</a>. The heart of machine learning is using mathematical models to allow computers to "see" patterns without being explicitly told how to do so, typically by training these models on huge datasets.</p>
<p>This technology has all kinds of applications, from <a href="https://towardsdatascience.com/how-do-self-driving-cars-see-13054aee2503" rel="nofollow">enabling self-driving cars</a> to <a href="https://www.theguardian.com/technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook" rel="nofollow">"deep" faking videos of politicians</a> to <a href="https://ai.google/research/pubs/pub46789" rel="nofollow">discovering new planets</a>.</p>
<p>But we just wanted to use it to make something fun. For kids.</p>
<p><a href="images/camera-onsite.png" target="_blank" rel="noopener noreferrer"><img src="images/camera-onsite.png" alt="onsite" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-the-concept" class="anchor" href="#the-concept" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The concept</h3>
<p>We created an installation for <a href="https://www.cinekid.nl/" rel="nofollow">CineKid</a>, an annual festival in Amsterdam that introduces children to media and media technologies.</p>
<p>We won a "Golden Lion" (not the one from Cannes, sorry) for the best project at the festival. Read some good press <a href="https://www.cinekid.nl/nl/nieuws/94" rel="nofollow">here</a> (in Dutch) and on the design site <a href="https://www.itsnicethat.com/articles/random-studio-body-image-search-digital-151118" rel="nofollow">It's Nice That</a> (in English).</p>
<p><a href="images/nice-that.png" target="_blank" rel="noopener noreferrer"><img src="images/nice-that.png" alt="article" style="max-width:100%;"></a></p>
<p>"Body" was about allowing kids to interact with something similar to a search engine, but using their own bodies as the interface, rather than a keyboard.</p>
<h3>
<a id="user-content-the-technology-challenge" class="anchor" href="#the-technology-challenge" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The technology challenge</h3>
<p>Our team had a very particular set of limitations (some of them self-imposed) which translated into some very interesting technical challenges for this project:</p>
<ul>
<li>effectively make use of a relatively large (for us) dataset of <strong>hundreds of thousands of images</strong>
</li>
<li>matching poses from a camera to poses from the image dataset using Machine Learning tools, <strong>but only using web-based technologies</strong>
</li>
<li>achieve a <strong>realtime, multi-user interactive experience</strong> that matched poses of kids in front of the screen to poses in our dataset - as close to instantaneously as possible</li>
</ul>
<p>It's important to note that none of our team members are data scientists; nor had any of us had significant experience applying "AI" or Machine Learning technologies in our daily work. As a team of creatives, designers, creative technologists and developers, some aspects of this project seemed familiar - graphics, projection, tracking people with sensors - but normally we don't need to deal with huge datasets or esoteric mathematics. This was new territory for us.</p>
<p>In our early research, one of the strongest alternatives to web technologies that we considered was <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose</a>, which provides really high performance realtime pose estimation using C++ and <a href="https://developer.nvidia.com/cuda-zone" rel="nofollow">CUDA</a> (Nvidia General-Purpose GPU processing). However, while it's "free for non-commercial use", there is a substantial "royalty" fee (USD $25,000 per year!) in order to use it on commercial projects.</p>
<p>But apart from the cost to use it (legitimately), we were genuinely interested in seeing how far we could go with web-only technologies. Could we get at least reasonable performance, and reasonable accuracy?</p>
<p>We set our limit to tracking 5 people simultaneously (adequate for the amount of space we had) which ended up giving us a reasonable framerate (around 10 detections per second) on a powerful GPU (Nvidia GTX 1080).</p>
<p>With a "native" solution like OpenPose we could probably have tracked 20 or more people at 30fps, using the exact same hardware. The downside, for us, would have been much slower development time: we only had one C++ programmer on the team, and other aspects of the system would have been much more difficult (for us) to integrate.</p>
<p>Some of the key features of our installation had been demonstrated in a Google project called <a href="https://experiments.withgoogle.com/move-mirror" rel="nofollow">Move Mirror</a>, with a few key differences: we wanted to <strong>track multiple people simultaneously</strong>, and the output would be <strong>large-format</strong> (a projection surface) with "life size" mirror imagery <strong>tracking your position</strong> in space. We also wanted to see if we could push past the 80,000 image dataset used in this example (we eventually processed about <strong>150,000 images</strong>, and got about 160,000 matchable poses out of these). Finally, we would draw human poses out of <strong>images with multiple people</strong>, not rely on images that only contained a single pose (so we would need to crop and re-centre later).</p>
<p>The Move Mirror team had not released any source code but had written a very informative <a href="https://medium.com/tensorflow/move-mirror-an-ai-experiment-with-pose-estimation-in-the-browser-using-tensorflow-js-2f7b769f9b23" rel="nofollow">blog post</a> about their process. We adopted some of the same key technologies, namely:</p>
<ol>
<li>
<a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet">PoseNet</a> - a pretrained model for pose detection that can run in the browser</li>
<li>
<a href="https://www.tensorflow.org/js" rel="nofollow">TensorFlow.js</a> - a library for training and/or applying Machine Learning models in JavaScript.</li>
<li>
<a href="https://github.com/fpirsch/vptree.js/tree/master">VPTreeJS</a> - a Javascript implementation of the Vantage-Point Tree "nearest neighbour" search algorithm. We forked <a href="https://github.com/RandomStudio/vptree.js?organization=RandomStudio&amp;organization=RandomStudio">our own version on Random Studio's GitHub</a> which allowed for generating proper JSON output.</li>
</ol>
<h3>
<a id="user-content-tensorflow" class="anchor" href="#tensorflow" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TensorFlow</h3>
<p>TensorFlow is a free and open source library originally developed by Google. Its main application is in Machine Learning.</p>
<p>Tensorflow JS cleverly uses the browser's own hardware-accelerated graphics API - WebGL - in order to leverage the GPU when doing its work: see <a href="https://www.tensorflow.org/js/guide/platform_environment" rel="nofollow">this article</a> for more detail.</p>
<p>Therefore, to get more performance when running "live", we simply used the biggest graphics card we had - in this case, the <a href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080/" rel="nofollow">NVIDIA GTX 1080</a>.</p>
<p><a href="images/gtx1080.jpg" target="_blank" rel="noopener noreferrer"><img src="images/gtx1080.jpg" alt="gtx1080" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-vp-what" class="anchor" href="#vp-what" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>VP What?</h3>
<p>A Vantage Point Tree (VP Tree) is a very clever way of taking a bunch of "points" (these could represent anything, e.g. 150K body poses in our case) and arranging them in a tree structure that allows for rapid searching of points that are closest to each other (in our case, poses that are "similar").</p>
<p><a href="images/vptree-viz.png" target="_blank" rel="noopener noreferrer"><img src="images/vptree-viz.png" alt="vptree" style="max-width:100%;"></a></p>
<p><em>A visualisation of a subdivided vantage point tree, as illustrated in <a href="https://fribbels.github.io/vptree/writeup" rel="nofollow">this excellent explanatory article</a>.</em></p>
<p>This approach means that we do not need to compare a new incoming pose (from our camera) with every single pose in our dataset. This is what made "realtime" interaction possible.</p>
<h3>
<a id="user-content-tracking-people" class="anchor" href="#tracking-people" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tracking people</h3>
<p>We often build installations that involve tracking people in space. The Microsoft Kinect is a common go-to solution. But a Kinect can't really be used to detect poses in still images, so we would be stuck having to apply a different detection method on the other end, and then translating between pose estimation systems.</p>
<p>Ideally, we wanted to use the exact same detection method for both the "real" people in front of the installation and the images in our dataset - this would make matching a lot more straightforward.</p>
<p>Image based machine learning pose detection was therefore an ideal solution in many ways. It requires no special hardware (e.g. depth-sensing or stereographic 3D cameras) in order to "see" human bodies. Assuming the model is well trained and efficient, it should be able to estimate poses in any image - whether that image came from a folder of curated downloads or a frame from a video camera. In our case, we used a simple USB webcam which provided a wide (90°) angle view, a <a href="https://www.pixijs.com/" rel="nofollow">Logitech C930e</a>.</p>
<p>Our experience was so good that we would probably consider image-based, Machine Learning-driven pose detection over specialised hardware solutions for many other tracking scenarios. The technique holds up well in poor lighting conditions, low resolutions and is not dependent on exotic hardware.</p>
<h3>
<a id="user-content-scraping-images" class="anchor" href="#scraping-images" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scraping images</h3>
<p>We needed to find a LOT of images of people that we could download without copyright violations, preferably automatically. And they needed to be suitable for kids (more on that later).</p>
<p>We settled on a few good sources:</p>
<ul>
<li>The <a href="http://human-pose.mpi-inf.mpg.de/" rel="nofollow">MPII Human Pose Dataset</a>, a curated set of images "systematically collected using an established taxonomy of every day human activities". It is used to benchmark pose detection algorithms. Perhaps because the source for the images is Youtube videos, it is also delightfully random.
<a href="images/random_activities.png" target="_blank" rel="noopener noreferrer"><img src="images/random_activities.png" alt="mpii" style="max-width:100%;"></a>
</li>
<li>Some <a href="https://www.flickr.com/" rel="nofollow">flickr</a> archives. A curious mix of imagery.
<a href="images/flickr.jpg" target="_blank" rel="noopener noreferrer"><img src="images/flickr.jpg" alt="flickr" style="max-width:100%;"></a>
</li>
<li>The free and publicly-accessible <a href="https://www.rijksmuseum.nl/en/search" rel="nofollow">Rijksmuseum Archive</a> which includes landscapes, portraits, paintings, sketches, cartoons and photographs (including photographs of sculptures).
<a href="images/rijksmuseum.jpg" target="_blank" rel="noopener noreferrer"><img src="images/rijksmuseum.jpg" alt="rijksmuseum" style="max-width:100%;"></a>
</li>
<li>A large collection of manually curated pictures of apes, including hominids, which we included for a laugh, since occasionally you would be matched to one of these - especially if you struck monkey poses.
<a href="images/apes-and-hominids.jpg" target="_blank" rel="noopener noreferrer"><img src="images/apes-and-hominids.jpg" alt="apes" style="max-width:100%;"></a>
</li>
</ul>
<p>We wrote our own scraping tools, using NodeJS, to crawl index pages and generate lists of original images through links, then downloaded them all. In the end we had 696,897 images (about 614GB) to work with. Not Google-scale, obviously, but a significant challenge for a team of creative technologists and web developers.</p>
<p>Typically less than half of the Rijkmuseum archive images contained human poses (or at least, poses that could be detected by PoseNet), but their inclusion made the installation much richer visually than would have been the case if we had stuck with contemporary images only.</p>
<p>The PoseNet detection was impressive, and managed to infer human poses with incomplete information...</p>
<p><a href="images/invisible-body.png" target="_blank" rel="noopener noreferrer"><img src="images/invisible-body.png" alt="incomplete-pose" style="max-width:100%;"></a></p>
<p>... and even within non-photographic representations such as line drawings:</p>
<p><a href="images/linedrawings-ok.png" target="_blank" rel="noopener noreferrer"><img src="images/linedrawings-ok.png" alt="linedrawings" style="max-width:100%;"></a></p>
<p>We could therefore find poses in a wide variety of styles:</p>
<p><a href="images/pose-detection-batches.gif" target="_blank" rel="noopener noreferrer"><img src="images/pose-detection-batches.gif" alt="animated-results" style="max-width:100%;"></a></p>
<h3>
<a id="user-content-processing-and-matching" class="anchor" href="#processing-and-matching" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Processing and matching</h3>
<p>To get all of this imagery and data into a usable form was one of the biggest challenges in this project. We designed (and refined) our own "data processing pipeline", illustrated below:</p>
<p><a href="images/diagram.png" target="_blank" rel="noopener noreferrer"><img src="images/diagram.png" alt="diagram" style="max-width:100%;"></a></p>
<p>The pipeline was mostly a combination of command line tools written in NodeJS.</p>
<p>Apart from resizing, every image had to be loaded into the browser to run the PoseNet / Tensorflow pose detection - we possibly could have done this using NodeJS, but it was easier for us to actually see the results in the browser. Running the pose detection on a few thousand images could take many hours depending on the hardware; another reason that using web technology was beneficial was that it made the software extremely portable between different machines, so we could run these processes in parallel.</p>
<p><a href="images/lots-data.gif" target="_blank" rel="noopener noreferrer"><img src="images/lots-data.gif" alt="lots-of-data" style="max-width:100%;"></a></p>
<p>Now we had a bunch of massive JSON files (50-150MB each) containing references to the original image files and lists of body parts with positions. Here is a small excerpt of a single entry in a single file:</p>
<pre><code>{
    "src": "000001163.jpg",
    "index": 0,
    "dimensions": [
        1000,
        562.5
    ],
    "poses": [
        {
            "index": 0,
            "poseNorm": {
                "score": 0.7557380725355709,
                "keypoints": [
                    {
                        "score": 0.9580614566802979,
                        "part": "nose",
                        "position": {
                            "x": 0.42552320626180173,
                            "y": 0.19028639878242465
                        }
                    },
                    // ... etc. etc. etc.
</code></pre>
<p>Next, we flattened the arrays of poses (listing poses with images, rather than images with poses), stripped out some whitespace and then concatenated these into even bigger JSON files, renumbering indexes as we went along. Our final version of this output was a 421MB JSON file with 148,833 objects in an array.</p>
<p>Finally, we had to build the Vantage Point Tree.</p>
<p>This was stretching NodeJS to some serious memory limits. Just to <em>load</em> the giant input array required a <a href="https://www.npmjs.com/package/stream-json" rel="nofollow">streaming JSON parser</a> and to write out the file required another <a href="https://www.npmjs.com/package/big-json" rel="nofollow">streaming JSON library</a> to output the huge object for the VP Tree.</p>
<p><a href="images/vptree-stats.png" target="_blank" rel="noopener noreferrer"><img src="images/vptree-stats.png" alt="stats" style="max-width:100%;"></a></p>
<p>Our final VP Tree object contained over 8 million nodes. The contents looked something like this:</p>
<p><a href="images/vptree-json.png" target="_blank" rel="noopener noreferrer"><img src="images/vptree-json.png" alt="vptree-raw" style="max-width:100%;"></a></p>
<p>Along the way, we also had to "normalise" all the pose data, i.e. crop to the pose inside each image and put it inside a standard-sized "box" so that all poses could be comparable no matter where they appeared within their original images. Here, another strength of web development tooling was evident: we could easily share code using modules (npm modules) between multiple "front end" (browser-based) applications and various NodeJS command-line or server-side tools. That made it much simpler to standardise our processing of pose data in precisely the same way throughout the pipeline.</p>
<h3>
<a id="user-content-manually-curating" class="anchor" href="#manually-curating" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Manually curating</h3>
<p>All of the above automation was essential, but there was a problem: we had to filter content to be suitable for kids. Some examples were just ridiculous - this one from the Rijksmuseum collection:</p>
<p><a href="images/farting-monkeys.jpg" target="_blank" rel="noopener noreferrer"><img src="images/farting-monkeys.jpg" alt="fmonkeys" style="max-width:100%;"></a></p>
<p>But of course we had to look for more serious issues: no images with nudity, violence or anything remotely disturbing. Historical photos sometimes portrayed racist imagery. Anatomical diagrams could present what looked like severed body parts. Some artists are fond of grotesque faces as subjects for portraiture. Medieval art is full of depictions of torture. And so on...</p>
<p>We couldn't risk having anybody accidentally land on something horrible by mistake. Since we do not have the resources of Google or Facebook, using an automated process was out of the question.</p>
<p>Our poor interns.</p>
<h3>
<a id="user-content-displaying" class="anchor" href="#displaying" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Displaying</h3>
<p>From a User Experience design perspective, this installation presented an interesting challenge of how to present the results of "matches" on screen (at a large scale) in a way that was both aesthetically interesting but also allowed the users (kids, remember) to understand what was happening, and what they needed to <em>do</em>.</p>
<p>Since we were picking poses out of images with multiple visible human bodies, we would need to crop in to the original image. This presented some interesting opportunities.</p>
<p>At first, we tried simply "joining the dots" and roughly cutting out the edges of the "skeleton". The results were curious:</p>
<p><a href="images/cutout2.png" target="_blank" rel="noopener noreferrer"><img src="images/cutout2.png" alt="cutout" style="max-width:100%;"></a></p>
<p>We also tried cutting out using bigger triangular shapes:</p>
<p><a href="images/cutout3.png" target="_blank" rel="noopener noreferrer"><img src="images/cutout3.png" alt="cutout" style="max-width:100%;"></a></p>
<p>And got more creative, joining all identified body points to all other points in a kind of mesh:</p>
<p><a href="images/mesh-examples.jpg" target="_blank" rel="noopener noreferrer"><img src="images/mesh-examples.jpg" alt="cutout" style="max-width:100%;"></a></p>
<p>Some of these results were interesting, but occasionally too abstract. And when images changed fast (a few times a second) then it became confusing. We settled on using a rectangular crop instead.</p>
<p><a href="images/projection.jpg" target="_blank" rel="noopener noreferrer"><img src="images/projection.jpg" alt="projection" style="max-width:100%;"></a></p>
<p>We also overlaid a simple line drawing of the <em>user's</em> skeleton, as detected via the camera, and made sure to align and scale this to fit over the corresponding pose in the matched image. This technique ultimately helped to make a strong connection between your own body's pose and the matched poses from the images.</p>
<p>Finally, we displayed rapidly-updating rows of the 10 closest matches per person along the top of the screen (otherwise unused space with a projection surface that was a few metres tall). This was real data, not mocked up, and reinforced the appreciation of how much work the computer was doing - and how quickly - to produce all of these results on screen.</p>
<p><a href="images/testing.gif" target="_blank" rel="noopener noreferrer"><img src="images/testing.gif" alt="testing" style="max-width:100%;"></a></p>
<p>The output was drawn in the browser, of course, and we used the <a href="https://www.pixijs.com/" rel="nofollow">PixiJS library</a> extensively to provide high performance GPU-accelerated drawing and 2D special effects (for example, the trailing effect).</p>
<h2>
<a id="user-content-live-hardware-and-software" class="anchor" href="#live-hardware-and-software" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Live hardware and software</h2>
<p>To make sure we squeezed maximum performance out of the tracking-and-matching application, we split the "live" installation system over two Linux PC's. Another advantage of web technology: developing on one platform (mostly Mac OS) and deploying on another (Ubuntu Linux) was painless, since NodeJS and Chrome provide identical environments in a sandbox regardless of Operating System.</p>
<p><a href="images/hardware.png" target="_blank" rel="noopener noreferrer"><img src="images/hardware.png" alt="hardware" style="max-width:100%;"></a></p>
<p>One PC ran only the tracking/matching on the webcam feed. This was a <a href="https://reactjs.org/" rel="nofollow">React</a> app to provide a useful GUI while configuring and debugging.</p>
<p><a href="images/tracker-onsite.png" target="_blank" rel="noopener noreferrer"><img src="images/tracker-onsite.png" alt="gui" style="max-width:100%;"></a></p>
<p>As an interesting aside, we experimented with using <a href="https://www.typescriptlang.org/" rel="nofollow">TypeScript</a> with React (not a common combination, but certainly well <a href="https://facebook.github.io/create-react-app/docs/adding-typescript" rel="nofollow">supported</a>). It proved invaluable in this project, since precision with data and algorithms was essential, and TypeScript <a href="https://dzone.com/articles/what-is-typescript-and-why-use-it" rel="nofollow">helped us</a> to proactively manage the complexity of the software.</p>
<h2>
<a id="user-content-summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h2>
<p>The key lessons we drew from this experience:</p>
<ol>
<li>AI, or Machine Learning in particular, can be used by non-experts for a variety of creative applications, provided some time is invested in researching the appropriate tools, pretrained models, etc.</li>
<li>Machine Learning for tasks such as body tracking is a viable replacement for specialised hardware such as depth cameras.</li>
<li>Large datasets may require designing custom workflows and automated tooling in order to manage effectively. This needs to be factored into development time.</li>
<li>"Web technologies" (modern browsers, NodeJS, web application frameworks) provide a viable development toolkit for using Machine Learning technology. This situation is likely to improve as web graphics and General-Purpose GPU web technologies mature (see <a href="https://medium.com/@babylonjs/webgpu-is-coming-to-babylon-js-c44f8065ac05" rel="nofollow">WebGPU</a> for example).</li>
<li>While web technologies have drawbacks (mostly in terms of performance), the advantages are numerous:
<ol>
<li>Teams can draw on existing web development skills, which practically means <em>there are more people to help with the project</em>
</li>
<li>GUI controls are easier to implement</li>
<li>Module systems in Javascript are mature, so incorporating third-party code or sharing internal code is much easier than in many other environments (particularly C++)</li>
<li>Data is much easier to handle, transform and generate using formats such as JSON</li>
<li>What you might lose in raw performance, you gain in being able to rapidly experiment, prototype, iterate and get things done</li>
</ol>
</li>
</ol>
<p><a href="images/trophy.jpg" target="_blank" rel="noopener noreferrer"><img src="images/trophy.jpg" alt="trophy" style="max-width:100%;"></a></p>

